{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05b12784",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Computer Vision and CNNs\n",
    "\n",
    "### Trinity 2021 - Week 6 - 2021.06.01\n",
    "### Lucas Kruitwagen\n",
    "DPhil, Geography and the Environment, Smith School of Enterprise and the Environment\n",
    "#### lucas.kruitwagen@gmail.com\n",
    "#### @lucaskruitwagen\n",
    "#### https://github.com/Lkruitwagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29230826",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\">\n",
    "<i class=\"fas fa-envelope fa-xs\"></i>\n",
    "<i class=\"fab fa-twitter fa-xs\"></i>\n",
    "<i class=\"fab fa-github fa-xs\"></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c56cb5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contents - Week 6\n",
    "\n",
    "1. Computer Vision Problems\n",
    "\n",
    "1. Machine Learning Approach\n",
    "\n",
    "1. Let's Code! TF+MNIST\n",
    "\n",
    "1. What are CNNs?\n",
    "\n",
    "1. History of CNNs\n",
    "\n",
    "1. Let's Code! TF+MNIST+CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a7746e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1. What is Computer Vision\n",
    "\n",
    "Core problems in computer vision:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Lkruitwagen/teaching/main/cv/assets/computer_vision_tasks.png\" alt=\"drawing\" style=\"display:inline\" width=\"800\"></img><sub>[1]</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7107ac25",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "... Information extraction from spatially-structured data (e.g. images, video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b14903",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example Applications\n",
    "\n",
    "Optical Character Recognition | Facial Detection | Pose Detection \n",
    " -- | ---------------- | -------------- \n",
    " <img src=\"https://raw.githubusercontent.com/Lkruitwagen/teaching/main/cv/assets/license_plate.jpeg\" alt=\"drawing\" style=\"display:inline\" width=\"300\"></img><sub>[2]</sub> |   <img src=\"https://raw.githubusercontent.com/Lkruitwagen/teaching/main/cv/assets/friends.gif\" alt=\"drawing\" style=\"display:inline\" width=\"300\"></img><sub>[3]</sub>  |   <img src=\"https://raw.githubusercontent.com/Lkruitwagen/teaching/main/cv/assets/dance.gif\" alt=\"drawing\" style=\"display:inline\" width=\"300\"></img><sub>[4]</sub>  \n",
    " \n",
    "Self-driving Vehicles | Anomaly Detection | Medical Imagery\n",
    "--------------------- | ----------------- | --------------\n",
    " <img src=\"https://raw.githubusercontent.com/Lkruitwagen/teaching/main/cv/assets/selfdriving.gif\" alt=\"drawing\" style=\"display:inline\" width=\"300\"></img><sub>[5]</sub>  | <img src=\"https://raw.githubusercontent.com/Lkruitwagen/teaching/main/cv/assets/anomaly.jpg\" alt=\"drawing\" style=\"display:inline\" width=\"300\"></img><sub>[6]</sub>  | <img src=\"https://raw.githubusercontent.com/Lkruitwagen/teaching/main/cv/assets/medical.png\" alt=\"drawing\" style=\"display:inline\" width=\"300\"></img><sub>[7]</sub> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858f40c5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### More examples from Climate Change + AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2249f3d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Remote Sensing Solar PV Facilities - A global inventory**\n",
    "\n",
    "Solar PV is a key technology for mitigating climate change while increasing energy access in the Global South. Coauthors and I have used ML with remote sensing imagery to search the entire planet for solar PV facilities and determine their installation dates - critical data for supporting policy, engineering, and planning.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Lkruitwagen/teaching/main/cv/assets/fig-1_samples.png\" alt=\"drawing\" style=\"display:inline\" height=\"500\"></img> \n",
    "\n",
    "<sub>Kruitwagen, L., Story, K., Friedrich, J., Buyers, L., Skillman, S., Hepburn, C. (2021) In peer review at _Nature_. Supported by DescartesLabs Inc., the World Resources Insistute, and computing grants from AWS and GCP.</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d738f2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Cloud Type Detection for causal inference of aersol effects**\n",
    "\n",
    "The planet's energy balance is sensitive to the reflectance of marine boundary layer clouds. Cloud reflectance is determined by its mesoscale structure. Anthropogenic aerosols cause transitions in these structures. Coauthors and I use unsupervised ML to characterise cloud types and then isolate aerosol causal effects.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Lkruitwagen/teaching/main/cv/assets/fdl.gif\" alt=\"drawing\" style=\"display:inline\" height=\"500\"></img> \n",
    "\n",
    "<sub>Christensen, M., Jones, W., Kusner, M., Kruitwagen, L., Pearce, T., Saengkyongam, S., Watson-Parris, D. (2020) *Aerosol Effects on Mesoscale Structures in Marine Boundary Layer Clouds*. Supported by the European Space Agency and the Frontier Development Lab.</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f38bce7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Flood Detection and Mapping**\n",
    "\n",
    "Timely flood mapping is crucial for emergency response efforts. Lightweight flood-mapping models can be implemented on spacecraft for streaming inference and alert systems.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Lkruitwagen/teaching/main/cv/assets/ml4cc.png\" alt=\"drawing\" style=\"display:inline\" height=\"500\"></img> \n",
    "\n",
    "<sub>Ahmed, N., Budd, S., Kruitwagen, L., Mateo-Garcia, G., Maynard-Reid, M., Praveen, S., Roth, N. (alph.) (2021) A Machine-Learning for Climate Change (ML4CC) project. Supported by Trillium Technologies Ltd and UNOSAT.</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de9d07c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Multispectral+Radar Self-Supervised Sensor Fusion**\n",
    "\n",
    "Self-supervised sensor fusion of Sentinel-1 synthetic-aperature radar and Sentinel-2 multispectral data for general purpose semantic embeddings, leading to a proliferation of low-data use cases.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Lkruitwagen/teaching/main/cv/assets/deepsentinel.png\" alt=\"drawing\" style=\"display:inline\" height=\"500\"></img> \n",
    "\n",
    "<sub>Kruitwagen, L. (2020) *DeepSentinel*. Supported by Microsoft AI for Earth and the European Space Agency.</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35877ac",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Image References**\n",
    "\n",
    "<sub>[1] Li, F, Johnson, J., Yeung, S. (2017) http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf</sub>\n",
    "<sub>[2] https://medium.com/@quangnhatnguyenle/detect-and-recognize-vehicles-license-plate-with-machine-learning-and-python-part-1-detection-795fda47e922</sub>\n",
    "<sub>[3] https://towardsdatascience.com/real-time-face-recognition-with-cpu-983d35cc3ec5</sub>\n",
    "<sub>[4] https://nanonets.com/blog/human-pose-estimation-2d-guide/</sub>\n",
    "<sub>[5] https://towardsdatascience.com/semantic-segmentation-popular-architectures-dff0a75f39d0</sub>\n",
    "<sub>[6] https://www.ricoh.com/technology/tech/073_imagerecognition</sub>\n",
    "<sub>[7] https://www.nature.com/articles/s41598-019-42557-4</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ee7d67",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2. Machine Learning Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1932b076",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Like other domains, we have some data, $X$, with which we want to predict some target, $Y$. We're looking for a function:\n",
    "\n",
    "$F(X,\\theta) = \\hat{Y} \\approx Y$\n",
    "\n",
    "We can use a neural network, parameterised by $\\theta$ as a universal approximator. As long as $F$ is differentiable, we can define a loss function $\\mathcal{L}(\\hat{Y}, Y)$ which we can minimise to find the values $\\tilde{\\theta}$ that maximises the likelihood function:\n",
    "\n",
    "$\\tilde{\\theta} = \\text{argmin } \\mathcal{L}(F(X,\\theta),Y)$\n",
    "\n",
    "Common loss functions are MSE (L2 loss) for regression problems and cross-entropy for classification problems. These loss functions are smooth and concave, so gradient descent can be used to solve for $\\tilde{\\theta}$ corresponding to the global minimum of $\\mathcal{L}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb1981d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We also want to ensure our function $F$ is generaliseable to $X'$ and $Y'$ not in $X$ and $Y$, i.e. that neither the model nor parameters have been overfit to the available data. Parameter overfit is mitigated by regularisation via random dropout. Model overfit is managed by retaining an out-of-sample _validation set_ alongside the _training set_, and _test set_.\n",
    "* **training set**: used to solve for parameters $\\tilde{\\theta}$\n",
    "* **validation set**: used to explore the hyperparameter space \n",
    "* **test set**: used to report the out-of-sample performance of the maxmimum likelihood estimator\n",
    "\n",
    "Sometimes training data from different distributions can be used to train a single model. Multiple training sets and additional _training-validation_ sets can be used, but a machine learning problem should use only a single validation set drawn from the same distribution of the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f787bc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Approach in Computer Vision\n",
    "\n",
    "##### A single sample $X$\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Lkruitwagen/teaching/main/cv/assets/sample_X.png\" alt=\"drawing\" style=\"display:inline\" width=\"800\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7232ec28",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "C: Channels; H: Height (pixels); W: Width (pixels)\n",
    "\n",
    "**NB:** Tensorflow: \"channels-last\", i.e. [H,W,C]; PyTorch: \"channels-first\", i.e. [C,H,W]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f458a41",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### A single target $Y$\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Lkruitwagen/teaching/main/cv/assets/sample_Y.png\" alt=\"drawing\" style=\"display:inline\" width=\"1000\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e881c92",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How it used to be done:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Lkruitwagen/teaching/main/cv/assets/traditional_cv.png\" alt=\"drawing\" style=\"display:inline\" width=\"800\"></img><sub>[8]</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cc989f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Feature engineering: mathematical transforms of the input image that the expert hypothesizes are important for the downstream task, e.g. edge detection, color, color gradiants\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Lkruitwagen/teaching/main/cv/assets/lecun_2015_features.png\" alt=\"drawing\" style=\"display:inline\" width=\"200\"></img><sub>[9]</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34413fae",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How it's done now:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Lkruitwagen/teaching/main/cv/assets/dl_cv.png\" alt=\"drawing\" style=\"display:inline\" width=\"800\"></img><sub>[8]</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddf9d54",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Image References**\n",
    "\n",
    "<sub>[8]: O'Mahony, N., Campbell, S., Carvalho, A., Harapanahalli, S., Velasco-Hernandez, G., Krpalkova, L., Riordan, D., Walsh, J. (2019) https://arxiv.org/abs/1910.13796</sub>\n",
    "<sub>[9]: LeCun, Y., Bengio, Y., Hinton G. (2015) https://www.nature.com/articles/nature14539</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d2a015",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3. Computer Vision Hello World: MNIST\n",
    "**M**odified **N**ational **I**nstitute of **S**tandards and **T**echnology database: 70,000 grayscale pictures of hand-written digits 0-9, 28x28px.\n",
    "\n",
    "The original problem: how to machine-read US zip codes on letter envelopes. Now a ML benchmark and teaching dataset.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Lkruitwagen/teaching/main/cv/assets/mnist.png\" alt=\"drawing\" style=\"display:inline\" width=\"600\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae30df4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Let's Code!\n",
    "\n",
    "Import dependencies and set up Jupyter environment. \n",
    "\n",
    "*best practises: separate built-ins, packages, and ML-libraries. Vertically align in-line comments.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a28618",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import os, sys                       # some built-ins \n",
    "\n",
    "import matplotlib.pyplot as plt      # visualisation\n",
    "import numpy as np                   # data maniputlations\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds   # built-in MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d852519",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*You may wish to also set up your Lab environment if possible. Some helpful commands:*\n",
    "\n",
    "Watch GPU loading: `watch nvidia-smi -i 1`\n",
    "\n",
    "Watch CPU and memory loading: `htop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e0e4ef",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices()    # let's check that TF is GPU-ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194e9831",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "### Load the MNIST dataset iterators\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'mnist',                      # TFDS has MNIST built-in\n",
    "    split=['train', 'test'],      # MNIST only has train and test data... fine for today\n",
    "    as_supervised=True,           # dataset generators include samples and labels \n",
    "    with_info=True,               # also return dataset metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5fce97",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "type(ds_train), type(ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206b091b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`ds_train` and `ds_test` are *generators*. They serve our data to the ML model, manage batchsize and parallelisation, and can be customised with augmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e785270",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "### Inspect our data\n",
    "sample_X, sample_Y = next(ds_train.as_numpy_iterator())             # get a single sample\n",
    "print ('Single sample:',type(sample_X), type(sample_Y), sample_X.shape, sample_Y.shape)\n",
    "\n",
    "sample_X, sample_Y = next(ds_train.batch(100).as_numpy_iterator())  # get a batch of 100 samples\n",
    "print ('Batch sample:',type(sample_X), type(sample_Y), sample_X.shape, sample_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f353321",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "### Visualisat our data\n",
    "fig, axs = plt.subplots(10,10,figsize=(8,8))\n",
    "axs = axs.flatten()                                           # flatten array of axes (10,10) -> (100,)\n",
    "for ii in range(100):\n",
    "    axs[ii].text(13,-1,str(sample_Y[ii]),color='k')           # annotate above axis\n",
    "    axs[ii].imshow(np.squeeze(sample_X[ii,:,:]), cmap='gray') # squeeze out the channel dimension\n",
    "    axs[ii].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd8e3f9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Let's build on what we know - multiclass classification with scikit learn\n",
    "\n",
    "We want a multi-class classifier because our labels are one of 10 digits. We can make one with an ensemble of one-vs-all classifiers conveniently implemented by sklearn. We can use a SVM for each class' classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa80cbe",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a928ca",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We have a nice small dataset that can fit in memory. Let's get it all from our TF datasets into numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a1b333",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_trn = np.array([_x for _x, _y in ds_train.as_numpy_iterator()])\n",
    "Y_trn = np.array([_y for _x, _y in ds_train.as_numpy_iterator()])\n",
    "X_test = np.array([_x for _x, _y in ds_test.as_numpy_iterator()])\n",
    "Y_test = np.array([_y for _x, _y in ds_test.as_numpy_iterator()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcf54c3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Image data usually comes in as Byte integers. Let's normalise our input data and change it to float. Float64 is a bit excessive, so let's go with Float32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00978982",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('raw data:',X_trn.max(), X_trn.min(), X_trn.dtype)\n",
    "X_trn, X_test = (X_trn/255.).astype(np.float32), (X_test/255.).astype(np.float32)\n",
    "print('normalised:',X_trn.max(), X_trn.min(), X_trn.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a05df3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our targets are current stored as categorical values represented by integers. We need to 'one-hot' encode them to a vector of [0,1] targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bc7fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = np.unique(Y_trn).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e4f5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(targets, n_classes):\n",
    "    return np.eye(n_classes)[targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee728fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_trn = one_hot_encode(Y_trn, n_classes)\n",
    "Y_test = one_hot_encode(Y_test, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f685a7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Last thing - we need to flatten our training data X. The classifier expects a 2D dataframe of [n_samples,m_features]. Each pixel-channel datum will be a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c5c43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trn = X_trn.reshape(X_trn.shape[0],-1)\n",
    "X_test = X_test.reshape(X_test.shape[0],-1)\n",
    "print (X_trn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caed945",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "### train our one-vs-all classifier\n",
    "classifier = OneVsRestClassifier(LinearSVC()).fit(X_trn, Y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ae27c1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "### run prediction on our test data\n",
    "Y_test_hat = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693d627c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# use np.argmax to return test data to categorical\n",
    "accuracy = (np.argmax(Y_test_hat, axis=1)==np.argmax(Y_test, axis=1)).sum()/Y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abc2253",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ced8ca",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A simple deep neural network with TF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93010f11",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As before, we need to normalise our data and cast it to Float32. We can use tf.dataset.map to map a normalising function over all our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fd2ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_mapper(sample, target):                     # sample and target are now tf tensors\n",
    "    return tf.cast(tf.squeeze(sample), tf.float32) / 255., target      # return the (image, label) tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f5e074",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ds_train.map(normalise_mapper, num_parallel_calls=tf.data.experimental.AUTOTUNE)   # AUTOTUNE - > allows TF to decide how many CPU processes to use\n",
    "ds_test = ds_test.map(normalise_mapper, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978dacd8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Configure the data pipeline:\n",
    "- *cache*: for a small dataset, read it only once and keep it in memory\n",
    "- *shuffle*: randomly select dataset elements. \n",
    "- *batch*: set the batch size\n",
    "- *prefetch*: allow the data pipeline to fetch samples while the model is running and updating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523da5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples) # Set the buffer to the full dataset for small datasets\n",
    "ds_train = ds_train.batch(128)                                    # 128 a nice power of 2\n",
    "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)       # allow TF to decide how many processes to prefect data with\n",
    "\n",
    "ds_test = ds_test.cache()\n",
    "ds_test = ds_test.shuffle(ds_info.splits['test'].num_examples)\n",
    "ds_test = ds_test.batch(128)\n",
    "ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a92dd6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Build a simple fully-connected neural network with two hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de29b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),   # a non-parameterised layer to flatten our image data for full-connection\n",
    "  tf.keras.layers.Dense(128,activation='relu'),    # a fully connected layer with ReLU activation\n",
    "  tf.keras.layers.Dropout(0.5),                    # a dropout layer for regularisation\n",
    "  tf.keras.layers.Dense(10)                        # an output layer with same dimension as our targets\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce165097",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),                                # ADAM optimizer -> always a good first bet\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),     # from_logits -> don't need to one-hot the targets\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fab9b9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    ds_train,\n",
    "    epochs=10,\n",
    "    validation_data=ds_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f6f6c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m68",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m68"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
